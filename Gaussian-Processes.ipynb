{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Gaussian Processes\n",
      "\n",
      "We now introduce the concept of Gaussian Processes. We look at GPs in this section from the regression point of view - later we will cover the classification problem where we try for instance to determine if a UE in indoors or not. Worth mentioning that the regression and traffic intensity mapping problem statements are quite different, however treatment of the regression problem is key to understanding the basic principle behind traffic intensity mapping. This is by no means a tutorial treatment - the interested reader is referred to Carl's Rasmussen video below.  \n",
      "\n",
      "<a href='http://videolectures.net/mlss09uk_rasmussen_gp/'>\n",
      "  <img src='http://videolectures.net/mlss09uk_rasmussen_gp/thumb.jpg' border=0/>\n",
      "  <br/>Gaussian Processes</a><br/>\n",
      "Carl Edward Rasmussen\n",
      "<br/><i><small>2\u00a0videos</small></i>\n",
      "\n",
      "## Introduction\n",
      "$$\\newcommand{\\vect}[1]{\\boldsymbol{#1}} % Uncomment for BOLD vectors.$$\n",
      "In regression problems we assume the presence of an input training data set \n",
      "$\\cal D = (\\vect y,\\vect{x})$ with the target to make predictions for the new inputs $\\vect{\\tilde{x}}$ included in the test data set.  \n",
      "\n",
      "### Traditional Bayesian Setting\n",
      "In the Bayesian setting, we start with a parametric model of the regression function $f(\\vect{w},\\vect{x})$ \n",
      "\n",
      "\\begin{equation}\n",
      "\\vect{y} = f(\\vect{w},\\phi(\\vect{x}))  + \\vect{n}\n",
      "\\end{equation}\n",
      "\n",
      "and this parametric model was assume to be, for example, a linear model of the form $f(\\vect{w},\\phi(\\vect{x}))=\\vect{w}^T \\phi(\\vect{x})$. Then we introduce a prior distribution $p(\\vect{w})$ of the model parameters that gave raise to the posterior parameter distribution,\n",
      "\n",
      "\\begin{equation}\n",
      "p(\\vect{w}|\\vect{x},\\vect{y}) = \\frac{p(\\vect{w})p(\\vect{y}|\\vect{x,w})}{p(\\vect{y}|\\vect{x})}\n",
      "\\end{equation} \n",
      "\n",
      "Making predictions involved a marginalizing out operation, over the posterior $p(\\vect{w}|\\vect{x},\\vect{y})$, i.e.\n",
      "\n",
      "\\begin{equation}\n",
      "p(\\tilde{y}_k|\\tilde{x}_k, \\vect{x,y}) = \\int p(\\tilde{y}_k|\\vect{w}, \\tilde{x}_k) p(\\vect{w}|\\vect{x},\\vect{y}) d\\vect{w}\n",
      "\\end{equation}\n",
      "\n",
      "In summary, with the parametric model, we are going through an indirect step to first introduce the prior distribution over the parameters and then, during the prediction step, remove its posterior by integration. The parametric model is summarized in  Table [Parametric vs Gaussian Process Models](#Parametric_vs_Gaussian_Process_Models), to make this indirect approach more evident and compare it to the non-parametric models. One alternative is to work directly in the regression function-space and introduce a probability distribution over *regression functions*, instead of, over random variables $p(\\vect{w})$.  \n",
      "\n",
      "## Defining a Gaussian Process\n",
      "Let us know walk through the transition from the weight-space of parametric Bayesian regression to the function-space of GPs. \n",
      "\n",
      "**Gaussian Process**\n",
      "> An infinite collection of random variables, any finite subset of which have a Gaussian distribution.\n",
      "\n",
      "A Gaussian process is a ***disribution over functions***. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP $f(\\vect{x}) = {\\cal GP}(m(\\vect{x}), k(\\vect{x},\\vect{x}'))$ is fully specified by \n",
      "its mean function $m(\\vect{x})$ and its covariance $k(\\vect{x},\\vect{x}')$ where,\n",
      "\n",
      "\\begin{eqnarray}\n",
      "m(\\vect{x}) &=& E[f(\\vect{x})] \\\\ \n",
      "k(\\vect{x},\\vect{x}') &=& E[(f(\\vect{x})-m(\\vect{x})) (f(\\vect{x}')-m(\\vect{x}'))] \n",
      "\\end{eqnarray}\n",
      "\n",
      "The $\\vect{x}'$ denotes an input covariate vector different than $\\vect{x}$ and $k(.)$ is a positive definite kernel. \n",
      "\n",
      "A GP can therefore be considered a generalization of a finite set ($N$) of jointly Gaussian random variables, as it involves an infinite set of variables i.e. a function. Conversely, a GP is a collection of random variables, any finite (N) number of which, has a consistent joint Gaussian distribution,\n",
      "\n",
      "\\begin{eqnarray} \n",
      "p(f|X) & = & {\\cal N}(\\vect{f}|\\vect{\\mu}, K) \\\\\n",
      "\\vect{\\mu} & = & \\left(\\mu(\\vect{x}_1), \\dots, \\mu(\\vect{x}_N)) \\right)^T \\\\\n",
      "K_{ij} & = & k(\\vect{x}_i, \\vect{x}_j) \n",
      "\\end{eqnarray}\n",
      "\n",
      "This is called the *marginalization property* of GPs and it is always used in practice to generate realizations of a Gaussian Process, as we always have a finite  $N$ to deal with.  The equations governing GP regression are shown in the corresponding column of Table [Parametric vs Gaussian Process Models](#Parametric_vs_Gaussian_Process_Models). In the case $\\sigma_n^2=0$ the same equations governs the interpolation problem that as we will see is the problem that underlines the traffic intensity mapping based on OSS data. Also, for analytical simplicity we assume that the likelihood function is Gaussian. Any other likelihood function is possible (e.g. Poisson) and the reader is encouraged not to relate in any way the Gaussian nature of the likelihood with the GP prior.\n",
      "\n",
      "<table> <a name=\"Parametric_vs_Gaussian_Process_Models\"></a>\n",
      "<caption> Comparison of Linear Bayesian and GP Models with Gaussian Likelihood $y=f(x)+\\epsilon$ </caption>\n",
      "<tr>\n",
      "<th></th>\n",
      "<th>Fully Parametric</th>\n",
      "<th>Gaussian Process</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Likelihood</td>\n",
      "<td>$p(\\vect{y}|\\vect{x},\\vect{w})$</td>\n",
      "<td>$p(\\vect{y}|\\vect{x},f(x)) \\sim {\\cal N}(\\vect{f}, \\sigma_n^2 I)$</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Marginal Lik.</td>\n",
      "<td>$p(\\vect{y}|\\vect{x}) = \\int p(\\vect{y}|\\vect{x},\\vect{w}) p(\\vect{w}) d\\vect{w}$</td>\n",
      "<td>$p(\\vect{y}|\\vect{x})$</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Prior</td>\n",
      "<td>$p(w)$</td>\n",
      "<td>$p(f(x)) \\sim {\\cal GP} \\left( m(\\vect{x}), k(\\vect{x},\\vect{x}') \\right)$</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Posterior</td>\n",
      "<td>$p(\\vect{w}|\\vect{x},\\vect{y})$</td>\n",
      "<td>\\begin{eqnarray} p(f(x)|\\vect{x},\\vect{y}) &\\sim& {\\cal GP}(m_{post}, k_{post}) \\\\\n",
      "m_{post}(x) & = & k(x,\\vect{x})[K(\\vect{x},\\vect{x}+\\sigma_n^2 \\vect{I}]^{-1}\\vect{y} \\\\\n",
      "k_{post}(x,x') & = & k(x,x') - k(x,\\vect{x})[K(\\vect{x},\\vect{x}) +\\sigma_n^2 \\vect{I}]^{-1}k(\\vect{x},x')) \n",
      "\\end{eqnarray}</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Predictive</td>\n",
      "<td>$p(y^*|x^*,\\vect{x},\\vect{y}) = \\int p(y^* | x^*, \\vect{w}) p(\\vect{w}|\\vect{x},\\vect{y})) d\\vect{w}$</td>\n",
      "<td>\\begin{eqnarray} \n",
      "p(y^*|x^*,\\vect{x},\\vect{y}) & \\sim &  {\\cal N} (m_{pred},\\Sigma_{pred}) \\\\ \n",
      "m_{pred} & = & K(X,X_*)^T [K(X,X) +\\sigma_n^2 \\vect{I}]^{-1}\\vect{y} \\\\\n",
      "\\Sigma_{pred} & = & K(X_*,X_*) - K(X,X_*)^T [K(X,X) + \\\\  \\nonumber\n",
      "& & \\sigma_n^2\\vect{I}]^{-1} K(X,X_*)) \n",
      "\\end{eqnarray}</td>\n",
      "</tr>\n",
      "</table>\n",
      "\n",
      "It should be noted that the term \"non-parametric\" in the context of Bayesian analysis is something of a misnomer. This is because the first and fundamental step in Bayesian modeling is to specify a *full probability model* for the problem at hand. It is rather difficult to explicitly state a full probability model without the use of probability functions, which are parametric. Bayesian non-parametric methods do not imply that there are no parameters, but rather that the number of parameters grows with the size of the dataset. In fact, Bayesian non-parametric models are *infinitely* parametric.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Generating Gausssian Processes\n",
      "The generation of Gaussian Processes is done using the marginalization propertry. For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean functon and covariance function evaluated at those points. \n",
      "\n",
      "First, the parameters of the GP must be defined. If we assume a zero mean GPs i.e. ${\\cal GP}(0, k(\\vect{x},\\vect{x}'))$ and a squared exponential (SE) covariance function, \n",
      "\n",
      "\\begin{equation}\n",
      "k(\\vect{x},\\vect{x}') = \\sigma_f^2 \\exp (-\\frac{1}{2} (\\vect{x}-\\vect{x}')^T M (\\vect{x}-\\vect{x}')) + \\sigma_n^2 \\delta_{xx'}\n",
      "\\end{equation}\n",
      "\n",
      "where $\\sigma_f^2$ controls the vertical scale and the matrix M controls the horizontal length scale. For example if $M=l^{-2}I$ then $l$ is the only control parameter of the isotropic horizontal length scale. Other definitions of M allow for arbitrary control of the horizontal length scale per dimension. The SE parameters  $\\sigma_f^2$ and $l$ (isotropic M) will need to be estimated, as we will see shortly.  \n",
      "\n",
      "The isotropic exponential kernel is defined in python:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "# Basic exponential kernel\n",
      "exponential_kernel = lambda x, y, params: params[0] * \\\n",
      "    np.exp( -0.5 * params[1] * sum((x - y)**2) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The covariance matrix is  referred to as the kernel (or Gram) matrix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Covariance matrix for MV normal\n",
      "covariance = lambda kernel, x, y, params: \\\n",
      "    np.array([[kernel(xi, yi, params) for xi in x] for yi in y])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "x = np.random.randn(10)*2\n",
      "theta = [1, 10]\n",
      "sigma = covariance(exponential_kernel, x, x, theta)\n",
      "y = np.random.multivariate_normal(np.zeros(len(x)), sigma)\n",
      "plot(x, y, 'bo')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'numpy.float64' object is not iterable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-7-87d402922ba0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcovariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexponential_kernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-6-e840a0d5e93f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(kernel, x, y, params)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Covariance matrix for MV normal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcovariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0myi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-5-acf9434354c9>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, y, params)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Basic exponential kernel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mexponential_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can generate sample functions (*realizations*) sequentially, using the conditional:\n",
      "\n",
      "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
      "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def conditional(x_new, x, y, fcov=exponential_kernel, params=theta):\n",
      "    B = covariance(fcov, x_new, x, params)\n",
      "    C = covariance(fcov, x, x, params)\n",
      "    A = covariance(fcov, x_new, x_new, params)\n",
      "    mu = np.linalg.inv(C).dot(B).T.dot(y)\n",
      "    sigma = A - np.linalg.inv(C).dot(B).T.dot(B)\n",
      "    return mu.squeeze(), sigma.squeeze()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma0 = exponential_kernel(0, 0, theta)\n",
      "xpts = arange(-3, 3, step=0.01)\n",
      "plt.errorbar(xpts, np.zeros(len(xpts)), yerr=sigma0, capsize=0)\n",
      "ylim(-3, 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'int' object is not iterable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-9-8aecf6c4ce47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msigma0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexponential_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mxpts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxpts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxpts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigma0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-5-acf9434354c9>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, y, params)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Basic exponential kernel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mexponential_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We start by selecting a point at random, then drawing from an *unconditional* Gaussian:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = [1.]\n",
      "y = [np.random.normal(scale=sigma0)]\n",
      "y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'sigma0' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-7055b425448f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigma0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'sigma0' is not defined"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now calculate the conditional distribution, given the point that we just sampled."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma1 = covariance(exponential_kernel, x, x, theta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'float' object is not iterable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-0dd538cb1843>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msigma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcovariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexponential_kernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-6-e840a0d5e93f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(kernel, x, y, params)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Covariance matrix for MV normal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcovariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0myi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-5-acf9434354c9>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, y, params)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Basic exponential kernel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mexponential_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def predict(x, data, kernel, params, sigma, t):\n",
      "    k = [kernel(x, y, params) for y in data]\n",
      "    Sinv = np.linalg.inv(sigma)\n",
      "    y_pred = np.dot(k, Sinv).dot(t)\n",
      "    sigma_new = kernel(x, x, params) - np.dot(k, Sinv).dot(k)\n",
      "    return y_pred, sigma_new"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_pred = np.linspace(-3, 3, 1000)\n",
      "predictions = [predict(i, x, exponential_kernel, theta, sigma1, y) \n",
      "               for i in x_pred]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'sigma1' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-13-424e7be871c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m predictions = [predict(i, x, exponential_kernel, theta, sigma1, y) \n\u001b[1;32m----> 3\u001b[1;33m                for i in x_pred]\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'sigma1' is not defined"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's plot this to get an idea of what this looks like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred, sigmas = np.transpose(predictions)\n",
      "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
      "plt.plot(x, y, \"ro\")\n",
      "xlim(-3, 3); ylim(-3, 3);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'predictions' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-14-529ca55d4607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxlim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, we can select a second point, conditional on the first point, using this new distribution. Let's arbitrarily select one at $x=-0.7$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mu2, s2 = conditional([-0.7], x, y)\n",
      "y2 = np.random.normal(mu2, s2)\n",
      "y2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'y' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-15-de4937ffaf53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmu2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconditional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x.append(-0.7)\n",
      "y.append(y2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'y' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-16-4f74ef03c0b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can calculate the conditional distribution again:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma2 = covariance(exponential_kernel, x, x, theta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'float' object is not iterable",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-17-044a107d534f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msigma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcovariance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexponential_kernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-6-e840a0d5e93f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(kernel, x, y, params)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Covariance matrix for MV normal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcovariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0myi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-5-acf9434354c9>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, y, params)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Basic exponential kernel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mexponential_kernel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions = [predict(i, x, exponential_kernel, theta, sigma2, y) \n",
      "               for i in x_pred]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'sigma2' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-18-775b7daed7b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m predictions = [predict(i, x, exponential_kernel, theta, sigma2, y) \n\u001b[1;32m----> 2\u001b[1;33m                for i in x_pred]\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'sigma2' is not defined"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred, sigmas = np.transpose(predictions)\n",
      "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
      "plt.plot(x, y, \"ro\")\n",
      "xlim(-3, 3); ylim(-3, 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'predictions' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-19-11737933dc26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrorbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxlim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice how the existing points constrain the selection of subsequent points."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_more = [-2.1, -1.5, 0.3, 1.8, 2.5]\n",
      "mu, s = conditional(x_more, x, y)\n",
      "y_more = np.random.multivariate_normal(mu, s)\n",
      "y_more"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'y' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-20-89b8c2298f1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mx_more\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconditional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_more\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_more\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_more\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x += x_more\n",
      "y += y_more.tolist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma_new = covariance(exponential_kernel, x, x, theta)\n",
      "\n",
      "predictions = [predict(i, x, exponential_kernel, theta, sigma_new, y) \n",
      "               for i in x_pred]\n",
      "\n",
      "y_pred, sigmas = np.transpose(predictions)\n",
      "plt.errorbar(x_pred, y_pred, yerr=sigmas, capsize=0)\n",
      "plt.plot(x, y, \"ro\")\n",
      "ylim(-3, 3);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that this is exactly equivalent to adding points simultaneously."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are not restricted to a double-exponential covariance function. Here is a slightly more general function, which includes a constant and linear term, in addition to the exponential.\n",
      "\n",
      "$$k(x,x\\prime) = \\theta_1 \\exp\\left(-\\frac{\\theta_2}{2}(x-x\\prime)^2\\right) + \\theta_3 + \\theta_4 x^T x^\\prime$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Exponential kernel, plus constant and linear terms\n",
      "exponential_linear_kernel = lambda x, y, params: \\\n",
      "    exponential_kernel(x, y, params[:2]) + params[2] + params[3] * np.dot(x, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Parameters for the expanded exponential kernel\n",
      "theta = 2.0, 50.0, 0.0, 1.0\n",
      "\n",
      "# Some sample training points.\n",
      "xvals = np.random.rand(10) * 2 - 1\n",
      "\n",
      "# Construct the Gram matrix\n",
      "C = covariance(exponential_linear_kernel, xvals, xvals, theta)\n",
      "\n",
      "# Sample from the multivariate normal\n",
      "yvals = np.random.multivariate_normal(np.zeros(len(xvals)), C)\n",
      "\n",
      "plt.plot(xvals, yvals, \"ro\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_pred = np.linspace(-1, 1, 1000)\n",
      "predictions = [predict(i, xvals, exponential_linear_kernel, theta, C, yvals) \n",
      "               for i in x_pred]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y, sigma = np.transpose(predictions)\n",
      "plt.errorbar(x_pred, y, yerr=sigma, capsize=0)\n",
      "plt.plot(xvals, yvals, \"ro\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use the GP as a prior, and update it using data (rather than random realizations), to obtain a posterior GP that we can use for prediction, conditional on the data.\n",
      "\n",
      "## Marginal Likelihood\n",
      "\n",
      "The marginal likelihood is the normalizing constant for the posterior distribution, and is the integral of the product of the likelihood and prior.\n",
      "\n",
      "$$p(y|X) = \\int_f p(y|f,X)p(f|X) df$$\n",
      "\n",
      "where for Gaussian processes, we are marginalizing over function values $f$ (instead of parameters $\\theta$).\n",
      "\n",
      "GP prior:\n",
      "\n",
      "$$\\log p(f|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K| -\\frac{1}{2}f^TK^{-1}f $$\n",
      "\n",
      "Gaussian likelihood:\n",
      "\n",
      "$$\\log p(y|f,X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|\\sigma^2I| -\\frac{1}{2}(y-f)^T(\\sigma^2I)^{-1}(y-f) $$\n",
      "\n",
      "Marginal likelihood:\n",
      "\n",
      "$$\\log p(y|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K + \\sigma^2I| - \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y $$\n",
      "\n",
      "Notice that the marginal likelihood includes both a data fit term $- \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y$ and a parameter penalty term $\\frac{1}{2}\\log|K + \\sigma^2I|$. Hence, the marginal likelihood can help us select an appropriate covariance function, based on its fit to the dataset at hand.\n",
      "\n",
      "### Choosing parameters\n",
      "\n",
      "This is relevant because we have to make choices regarding the parameters of our Gaussian process; they were chosen arbitrarily for the random functions we demonstrated above.\n",
      "\n",
      "For example, in the squared exponential covariance function, we must choose two parameters:\n",
      "\n",
      "$$k(x,x\\prime) = \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^\\prime)^2\\right)$$\n",
      "\n",
      "The first parameter $\\theta_1$ is a scale parameter, which allows the function to yield values outside of the unit interval. The second parameter $\\theta_2$ is a length scale parameter that determines the degree of covariance between $x$ and $x^\\prime$; smaller values will tend to smooth the function relative to larger values.\n",
      "\n",
      "We can use the marginal likelihood to select appropriate values for these parameters, since it trades off model fit with model complexity. Thus, an optimization procedure can be used to select values for $\\theta$ that maximize the marginial likelihood."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These examples, of course, are trivial because they are simply random functions. What we are really interested in is *learning* about an underlying function from information residing in our data. In a parametric setting, we either specify a likelihood, which we then maximize with respect to the parameters, of a full probability model, for which we calculate the posterior in a Bayesian context. Though the integrals associated with posterior distributions are typically intractable for parametric models, they do not pose a problem with Gaussian processes.\n",
      "\n",
      "## Gaussian process priors\n",
      "\n",
      "We can treat our zero-mean (or otherwise arbitrary) Gaussian process as a prior for our model. If we are able to use a Gaussian as our data likelihood, then we can construct a Gaussian proceess posterior.\n",
      "\n",
      "Keeping in mind that a Gaussian process is a distribution over functions, rather than parameters, our likelihood takes the following form:\n",
      "\n",
      "$$y|p(x),x \\sim \\mathcal{N}(p(x), \\sigma^2I) $$\n",
      "\n",
      "Here, $\\sigma^2$ represents observation error, or noise, and our unknown is a function.\n",
      "\n",
      "Notice that the GP likelihood conditions on a function; however, we are only interested in the function at locations where we have data!\n",
      "\n",
      "The corresponding prior is:\n",
      "\n",
      "$$p(x) \\sim \\mathcal{GP}(m_0(x), k_0(x,x\\prime))$$\n",
      "\n",
      "Multiplying an infinite normal with another infinite normal yields another infinite normal, our posterior process:\n",
      "\n",
      "$$p(x)|y \\sim \\mathcal{GP}(m_{post}, k_{post}(x,x\\prime))$$\n",
      "\n",
      "where\n",
      "\n",
      "$$\\begin{aligned}\n",
      "m_{post} &= k(x,x\\prime)^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
      "k_{post}(x,x\\prime) &= k(x\\prime,x\\prime) - k(x,x\\prime)^T[k(x,x) + \\sigma^2I]^{-1}k(x,x\\prime)\n",
      "\\end{aligned}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The posterior predictive distribution for the GP is:\n",
      "\n",
      "$$\\begin{aligned}\n",
      "m^* &= k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
      "k^*(x^*,x) &= k(x^*,x^*)+\\sigma^2 - k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}k(x^*,x)\n",
      "\\end{aligned}$$\n",
      "\n",
      "Though this calculation may seem straightforward, but note that the mean and covariate calculations involve inversions $k(x,x)$, which is a $\\mathcal{O}(n^3)$ computation. Thus, Gaussian processes as presented are usually only feasible for data up to a few thousand observations in size."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gaussian Processes in PyMC\n",
      "\n",
      "Along with standard parametric Bayesian models, PyMC 2 includes a module `gp` for fitting non-parametric models using Gaussian processes.\n",
      "\n",
      "### Mean function\n",
      "\n",
      "The mean function of a GP can be interpreted as a \"prior guess\" at the form of the true function. Typically, we use a zero mean function, as we have seen above, but we can choose from a range of alternatives."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymc import gp\n",
      "\n",
      "# Generate mean\n",
      "def quadfun(x, a, b, c):\n",
      "    return (a*x**2 + b*x + c)\n",
      "\n",
      "M = gp.Mean(quadfun, a=1., b=0.5, c=2.)\n",
      "\n",
      "x = np.arange(-1,1,0.1)\n",
      "plt.plot(x, M(x), 'k-')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Covariance function\n",
      "\n",
      "The behavior of individual realizations from the GP is governed by the covariance function. The Mat&#232;rn class of functions is a flexible choice.\n",
      "\n",
      "$$k_{Matern}(d) = \\frac{\\sigma^2}{\\Gamma(\\nu)2^{\\nu-1}} \\left(\\frac{\\sqrt{2 \\nu} d}{l}\\right)^{\\nu} K_{\\nu}\\left(\\frac{\\sqrt{2 \\nu} d}{l}\\right)$$\n",
      "\n",
      "The Mat&#232;rn covariance function has three parameters, each of which clearly controls one of three important properties of realizations.\n",
      "\n",
      "* amplitude ($\\sigma$)\n",
      ": The amplitude parameter (`amp` in PyMC) is a multiplier for realizations from the function that essentially stretches or compresses the y-axis.\n",
      "\n",
      "* lengthscale of changes ($l$)\n",
      ": The lengthscale parameter (`scale`) similarly scales realizations on the x-axis. Larger (greater than 1) values make points appear closer together.\n",
      "\n",
      "* roughness ($\\nu$)\n",
      ": The roughness parameter (`diff_degree`) controls the sharpness of the ridge of the covariance function, which in turn affects the roughness (or smoothness) of realizations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymc.gp.cov_funs import matern\n",
      "\n",
      "C = gp.Covariance(eval_fun=matern.euclidean, diff_degree=1.4, amp=0.4, \n",
      "                  scale=1, rank_limit=1000)\n",
      "\n",
      "plt.subplot(1,2,2)\n",
      "plt.contourf(x, x, C(x,x).view(ndarray), origin='lower', extent=(-1,1,-1,1), \n",
      "            cmap=cm.bone)\n",
      "plt.colorbar()\n",
      "\n",
      "plt.subplot(1,2,1)\n",
      "plt.plot(x, C(x,0).view(ndarray), 'k-')\n",
      "plt.ylabel('C(x,0)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Returns the diagnonal\n",
      "C(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Drawing realizations from a GP\n",
      "\n",
      "Since a Gaussian process is a distribution over functions, sampling from it yields functions rather than points. We refer to draws from GPs as *realizations*, which are represented using `Realization` objects in PyMC."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate realizations\n",
      "f_list = [gp.Realization(M, C) for i in range(3)]\n",
      "\n",
      "# Plot mean and covariance\n",
      "x = np.arange(-1,1,0.01)\n",
      "gp.plot_envelope(M, C, x)\n",
      "\n",
      "# Add realizations\n",
      "for f in f_list:\n",
      "    plt.plot(x, f(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use a GP to model observations as coming from a function for which we are highly uncertain. We can model this data as:\n",
      "\n",
      "$$ \\text{data}_i \\sim \\text{N}(f(o_i), V_i) $$\n",
      "\n",
      "which assumes only that the observation error is normally distributed. To represent the uncertainty regarding the expected value, we use a Gaussian process prior: \n",
      "\n",
      "$$ f \\sim \\text{GP}(M,C) $$ \n",
      "\n",
      "Combining these yields a posterior for *f* that is also a Gaussian process, with new mean and covariance functions:\n",
      "\n",
      "$$ f|\\text{data} \\sim \\text{GP}(M_o, C_o) $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = gp.Mean(quadfun, a=1., b=0.5, c=2.)\n",
      "C = gp.Covariance(eval_fun=matern.euclidean, diff_degree=1.4, \n",
      "                  amp=0.4, scale=1, rank_limit=1000)\n",
      "\n",
      "obs_x = np.array([-.5, .5])\n",
      "V = np.array([0.002, 0.002])\n",
      "data = np.array([3.1, 2.9])\n",
      "\n",
      "gp.observe(M=M, C=C, obs_mesh=obs_x, obs_V=V, obs_vals=data)\n",
      "\n",
      "# Generate realizations from posterior\n",
      "f_list = [gp.Realization(M,C) for i in range(3)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The function `observe` informs the mean and covariance functions that values on `obs_mesh` with observation variance `V`. Making observations with no error is called `conditioning`. This is useful when, for example, forcing a rate function to be zero when a population's size is zero."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gp.plot_envelope(M, C, mesh=x)\n",
      "for f in f_list:\n",
      "    plt.plot(x, f(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Example: Sockeye salmon spawning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sockeye_data = np.reshape([2986,9,\n",
      "3424,12.39,\n",
      "1631,4.5,\n",
      "784,2.56,\n",
      "9671,32.62,\n",
      "2519,8.19,\n",
      "1520,4.51,\n",
      "6418,15.21,\n",
      "10857,35.05,\n",
      "15044,36.85,\n",
      "10287,25.68,\n",
      "16525,52.75,\n",
      "19172,19.52,\n",
      "17527,40.98,\n",
      "11424,26.67,\n",
      "24043,52.6,\n",
      "10244,21.62,\n",
      "30983,56.05,\n",
      "12037,29.31,\n",
      "25098,45.4,\n",
      "11362,18.88,\n",
      "24375,19.14,\n",
      "18281,33.77,\n",
      "14192,20.44,\n",
      "7527,21.66,\n",
      "6061,18.22,\n",
      "15536,42.9,\n",
      "18080,46.09,\n",
      "17354,38.82,\n",
      "17301,42.22,\n",
      "11486,21.96,\n",
      "20120,45.05,\n",
      "10700,13.7,\n",
      "12867,27.71,], (34,2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abundance = sockeye_data[:,0].ravel()\n",
      "frye = sockeye_data[:,1].ravel()\n",
      "\n",
      "# Function for prior mean\n",
      "line = lambda x, slope: slope * x\n",
      "\n",
      "M = gp.Mean(line, slope=(frye / abundance).mean())\n",
      "\n",
      "C = gp.Covariance(matern.euclidean,\n",
      "            diff_degree=1.4,\n",
      "            scale=100.*abundance.max(),\n",
      "            amp=200.*frye.max())\n",
      "\n",
      "#gp.observe(M, C, obs_mesh=0, obs_vals=0, obs_V=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Observe some data\n",
      "gp.observe(M, C, obs_mesh=abundance, \n",
      "        obs_vals=frye, obs_V=10*frye)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xplot = np.linspace(0,1.25 * abundance.max(),100)\n",
      "\n",
      "gp.plot_envelope(M, C, xplot)\n",
      "\n",
      "for i in range(50):\n",
      "    f = gp.Realization(M, C)\n",
      "    plt.plot(xplot,f(xplot),alpha=0.3)\n",
      "\n",
      "plt.plot(abundance, frye, 'k.', markersize=4)\n",
      "plt.xlabel('Female abundance')\n",
      "plt.ylabel('Frye density')\n",
      "plt.title('Sockeye salmon recritment')\n",
      "plt.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gaussian Process Prior and MCMC\n",
      "\n",
      "The flexibility of Gaussian processes compels us to consider them as functional forms for prior distributions, and to fit them using MCMC. However, since a `GaussianProcess` object is a stochastic whose value is a `Realization` object, we cannot simply endow a it with a `logp` attribute, and so `GaussianProcess` objects cannot be handled by PyMC\u2019s standard MCMC machinery. However, we can evaluate the GP on a `obs_mesh`, which yields a simple multivariate normal random variable that can be handled by PyMC. Thus, the `GPSubmodel` class is a container for `GaussianProcess` objects that performs this step."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pymc import Gamma, deterministic, observed, MCMC\n",
      "from pymc import InverseGamma, Uniform, normal_like\n",
      "\n",
      "class SalmonSampler(MCMC):\n",
      "\n",
      "    def __init__(self, abundance, frye):\n",
      "\n",
      "        lfrye = log(frye)\n",
      "        labundance = log(abundance)\n",
      "\n",
      "        rx = labundance.max() - labundance.min()\n",
      "        ry = lfrye.max() - lfrye.min()\n",
      "\n",
      "\n",
      "        self.abundance = abundance\n",
      "        self.frye = frye\n",
      "        self.lfrye = np.log(frye)\n",
      "        self.labundance = np.log(abundance)\n",
      "        self.plot_x = np.linspace(self.abundance.min()*.1,self.abundance.max(),100)\n",
      "\n",
      "        # The mean function's parameters\n",
      "        beta_0 = Gamma('beta_0', alpha = log(4.5), beta = 1./(10.*(ry/4.)**2))\n",
      "        beta_1 = Gamma('beta_1', alpha = 1.6 * log(1000.) / ry**2, beta = 1.6 * log(1000.) / ry**2)\n",
      "\n",
      "        # The covariance function's parameters\n",
      "        invtausq = Gamma('invtausq', alpha = 2., beta = 1./(10.*(ry/4.)**2))\n",
      "\n",
      "        @deterministic\n",
      "        def amp(invtausq=invtausq):\n",
      "            \"\"\"\n",
      "            Prior amplitude of f.\n",
      "            \"\"\"\n",
      "            return 1./np.sqrt(invtausq)\n",
      "\n",
      "        scale = InverseGamma('scale' , alpha=2., beta=1./(6. / rx), value=3)\n",
      "        diff_degree = Uniform('diff_degree', .1, 3, value=1.5)\n",
      "\n",
      "        @deterministic\n",
      "        def C(diff_degree=diff_degree, amp=amp, scale=scale):\n",
      "            \"\"\"\n",
      "            The Matern covariance function, observed to be zero at the origin.\n",
      "            \"\"\"\n",
      "            C = gp.Covariance(matern.euclidean, diff_degree = diff_degree, amp = amp, scale = scale)\n",
      "            return C\n",
      "\n",
      "        @deterministic\n",
      "        def M(beta_0 = beta_0, beta_1 = beta_1):\n",
      "            \"\"\"\n",
      "            The mean function is the Cushing stock-recruitment function\n",
      "            \"\"\"\n",
      "            M = gp.Mean(lambda x: beta_0+ x*beta_1)\n",
      "            return M\n",
      "\n",
      "        SR = gp.GPSubmodel('SR', M, C, mesh = labundance)\n",
      "\n",
      "        frye_tau = Gamma('frye_tau', alpha = 2., beta = 1./(10.*(ry/4.)**2))\n",
      "\n",
      "        @deterministic\n",
      "        def frye_V(frye_tau=frye_tau):\n",
      "            \"\"\"\n",
      "            frye_V = 1/(frye_tau)\n",
      "            \"\"\"\n",
      "            return 1./(frye_tau)\n",
      "\n",
      "\n",
      "        @observed\n",
      "        def obs_frye(value=lfrye, mu = SR.f_eval, mesh=labundance, tau = frye_tau):\n",
      "            \"\"\"\n",
      "            The log of the frye count.\n",
      "            \"\"\"\n",
      "            return normal_like(value, mu, tau)\n",
      "\n",
      "        MCMC.__init__(self, locals())\n",
      "        \n",
      "        self.use_step_method(gp.GPEvaluationGibbs, SR, frye_V, obs_frye, verbose=0)\n",
      "\n",
      "\n",
      "    def plot_traces(self):\n",
      "        for object in [self.beta_0, self.beta_1, self.amp, self.scale, self.diff_degree, self.frye_tau]:\n",
      "            try:\n",
      "                y=object.trace()\n",
      "            except:\n",
      "                print object.__name__\n",
      "                break\n",
      "\n",
      "            plt.figure()\n",
      "            plt.plot(y)\n",
      "            plt.title(object.__name__)\n",
      "\n",
      "\n",
      "    def plot_SR(self):\n",
      "        f_trace = self.SR.f.trace()\n",
      "        \n",
      "        gp.gpplots.plot_GP_envelopes(self.SR.f, self.plot_x, transx = log, transy=exp)\n",
      "\n",
      "        for i in range(3):\n",
      "            plt.plot(self.plot_x, exp(f_trace[i](log(self.plot_x))), label='draw %i'%i)\n",
      "\n",
      "        plt.plot(self.abundance, self.frye, 'k.', label='data', markersize=8)\n",
      "        plt.axis([self.abundance.min()*.1, self.abundance.max(), 0., self.frye.max()*2.])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ss = SalmonSampler(abundance, frye)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ss.sample(10000, 5000, verbose=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ss.plot_SR()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ss.plot_traces()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gaussian Processes in `scikit-learn`\n",
      "\n",
      "`scikit-learn` also provides a robust implementation of Gaussian processes, with the same familiar interface that its other machine learning methods use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.gaussian_process import GaussianProcess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Specification of a `GaussianProcess` model class corresponds closely to that for PyMC. In place of `gp.Mean` and `gp.Covariance`, the scikit-learn implementation takes optional `regr` and `corr` arguments, respectively. These can either be callable functions or one of several built-in options.\n",
      "\n",
      "The `regr` argument specifies a regression function returning an array of outputs of the linear regression functional basis. The built-in functions are `constant` (default choice), `linear`, and `quadratic`.\n",
      "\n",
      "Similarly, the `corr` argument specifies a stationary autocorrelation function for any two points. The built-in correlation models include `absolute_exponential`, `squared_exponential` (default choice), `generalized_exponential`, `cubic`, and `linear`.\n",
      "\n",
      "Users can optionally specify autocorrelation paramter(s) `theta0`, which can either be scalar, or the size of the number of predictor variables. If arguments `thetaL` and `thetaU` are also provided, then these are taken as the lower and upper bounds on the autocorrelation parameters, with `theta0` the starting \"guess\" at a MLE of the parameter set.\n",
      "\n",
      "A `GaussianProcess` can be regularized by providing a `nugget` argument. Mathematically, the value represents the variance of the input values (*e.g.* observation error or other noise).\n",
      "\n",
      "Thus, the fitting of the regression and autocorrelation functions for the Gaussian process is highly automated in `scikits-learn`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = abundance[:, None]\n",
      "y = frye\n",
      "\n",
      "G = GaussianProcess(theta0=1e-3, nugget=1e-12)\n",
      "G.fit(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_pred = np.linspace(X.min(), X.max())[:, None]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred, MSE = G.predict(X_pred, eval_MSE=True)\n",
      "sigma = np.sqrt(MSE)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(X, y, 'r.', markersize=6, label=u'Observations')\n",
      "plt.plot(X_pred, y_pred, 'k:', label=u'Prediction')\n",
      "plt.fill(np.concatenate([X_pred, X_pred[::-1]]),\n",
      "        np.concatenate([y_pred - 1.9600 * sigma,\n",
      "                       (y_pred + 1.9600 * sigma)[::-1]]),\n",
      "        alpha=.3, fc='k', ec='None', label='95% confidence interval')\n",
      "plt.xlabel('Female abundance')\n",
      "plt.ylabel('Frye density')\n",
      "plt.legend(loc='upper left')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Example: Nashville daily temperatures\n",
      "\n",
      "The file `TNNASHVI.txt` in your data directory contains daily temperature readings for Nashville, courtesy of the [Average Daily Temperature Archive](http://academic.udayton.edu/kissock/http/Weather/). This data, as one would expect, oscillates annually. Use a Gaussian process to fit a regression model to this data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "daily_temps = pd.read_table(\"../data/TNNASHVI.txt\", sep='\\s+', \n",
      "                            names=['month','day','year','temp'], na_values=-99)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "temps_2010 = daily_temps.temp[daily_temps.year>2010]\n",
      "temps_2010.plot(style='b.', figsize=(10,6), grid=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Write your answer here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## References\n",
      "\n",
      "[Rasmussen, C. E., & Williams, C. K. I. (2005). Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning series). The MIT Press.](http://www.amazon.com/books/dp/026218253X)\n",
      "\n",
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}